---
# General setup VDO and Gluster volumes and services export bricks
# v1.00 20210301
# file: /roles/task_gluster_server.yml
# Each host will have optimal mount strucure of "itself first" then other nodes within gluster. Not sure how to do this yet
# Not sure how to build below playbook to mount and get options in /etc/fstab like below example
# UUID=13d50937-da1c-479a-a15a-ab2219d0ec8f /gluster_bricks/engine xfs inode64,noatime,nodiratime,_netdev,x-systemd.device-timeout=0,x-systemd.requires=vdo.service 0 0
# UUID=bf8358d8-21de-4f83-bb1b-dc9860465af6 /gluster_bricks/data xfs inode64,noatime,nodiratime,_netdev,x-systemd.device-timeout=0,x-systemd.requires=vdo.service 0 0
# UUID=1d60f87b-8f76-483e-9466-d87ec6ed3706 /gluster_bricks/vmstore xfs inode64,noatime,nodiratime,_netdev,x-systemd.device-timeout=0,x-systemd.requires=vdo.service 0 0
# UUID=8d2ee5e7-8f31-4ddd-8b25-efd58173c537 /gluster_bricks/gv0 auto _netdev,x-systemd.device-timeout=0,x-systemd.requires=vdo.service 0 0
# Working but needs feature:    
# Documentation https://docs.ansible.com/ansible/latest/collections/gluster/gluster/gluster_volume_module.html 
################### Black List for each Host so oVirt engine can manage with disk ########
# Thor multipath blacklist
# blacklist {
#         devnode "^sd[a-z]"
#         wwid Samsung_SSD_850_PRO_512GB_S250NXAGA15787L
#         wwid WDC_WDS100T2B0B-00YS70_19106A802926
#         wwid WDC_WDS100T2B0B-00YS70_192490801828
#         protocol "(scsi:adt|scsi:sbp)"
# }


# # odin multipath blacklist
# blacklist {
#         devnode "^sd[a-z]"
#         wwid Micron_1100_MTFDDAV512TBN_17401F699137
#         wwid WDC_WDS100T2B0B-00YS70_183533804564
#         wwid nvme.126f-4141303030303030303030303030303032343538-53504343204d2e32205043496520535344-00000001
#         protocol "(scsi:adt|scsi:sbp)"
# }

# # medusa multipath blacklist
# blacklist {
#         devnode "^sd[a-z]"
#         wwid SAMSUNG_SSD_PM851_mSATA_512GB_S1EWNYAF609306
#         wwid WDC_WDS100T2B0B-00YS70_19106A800900
#         wwid WDC_WDS100T2B0B_2013EP442601
#         protocol "(scsi:adt|scsi:sbp)"
# }

# ### install gluster server and vdo 
# - name: install gluster client and tools
#   yum:
#     name: 
#     - glusterfs-server
#     - vdo
#     state: latest
#   become: true

#### HCI Cluster ignites this structure and no ansible means to do this at this time  #####
# - name: Create Folder brick mount /gluster_bricks/data
#   file: 
#     path: /gluster_bricks/data
#     owner: root 
#     group: root 
#     mode: 0775 
#     state: directory
# - name: Create Folder  brick mount /gluster_bricks/vmstore
#   file: 
#     path: /gluster_bricks/vmstore
#     owner: root 
#     group: root 
#     mode: 0775 
#     state: directory
# - name: Create Folder /gluster_bricks/engine
#   file: 
#     path: /gluster_bricks/engine
#     owner: root 
#     group: root 
#     mode: 0775 
#     state: directory
#### HCI Cluster ignites this structure and no ansible means to do this at this time  #####

# ###  Create data00 volume with glusterfs 3 brick dedup compression  ####
# - name: Create Folder /gluster_bricks/data00
#   file: 
#     path: /gluster_bricks/data00
#     owner: root 
#     group: root 
#     mode: 0775 
#     state: directory

# # Create VDO volumes.  This will genearte device /dev/mapper/vdo_data00
# Broken: performance issue with OCP so dropping VDO for data00
# - name: Create 4 TB VDO volume data00 for oVirt vms
#   vdo:
#     name: vdo_data00
#     state: present
#     device: "/dev/mapper/{{ data00_disk }}"
#     logicalsize: 4T

# # Create partition on volume
# Broken: performance issue with OCP so dropping VDO for data00
# - name: Create partition
#   parted:
#     label: gpt
#     state: present
#     device: /dev/mapper/vdo_data00
#     number: 1
#     name: data00
#     flags: [ lvm ]
#     part_start: 0%
#     part_end: 90%
#     unit: '%'

# Create partition on volume
- name: Create partition
  parted:
    label: gpt
    state: present
    device: /dev/mapper/{{data00_disk}}
    number: 1
    name: data00
    flags: [ lvm ]
    part_start: 0%
    part_end: 95%
    unit: '%'

# # Format Partition with xfs 
# # Broken: performance issue with OCP so dropping VDO for data00
# - name: Create a xfs filesystem on VDO data00 volume label it but do not run discard blocks as it is not supported
#   filesystem:
#     fstype: xfs
#     dev: /dev/sdb1
#     opts: 
#       -L data00p1 -K

# Format Partition with xfs 
- name: Create a xfs filesystem on VDO data00 volume label it but do not run discard blocks as it is not supported
  filesystem:
    fstype: xfs
    dev: /dev/mapper/{{data00_disk}}p1
    opts: 
      -L data00 -K -f

# # Mount new VDO volume data00 
# # Broken: performance issue with OCP so dropping VDO for data00
# - name: Mount up device by label
#   mount:
#     path: /gluster_bricks/data00
#     src: LABEL=vdo_data00p1
#     opts: rw,relatime,seclabel,attr2,inode64,logbufs=8,logbsize=32k,noquota
#     fstype: xfs
#     state: mounted

# Mount new volume data00 
- name: Mount up device by label
  mount:
    path: /gluster_bricks/data00
    src: LABEL=data00
    opts: rw,relatime,seclabel,attr2,inode64,logbufs=8,logbsize=32k,noquota
    fstype: xfs
    state: mounted

###  Create data00/data00 folder for gluster to consume
- name: Create Folder /gluster_bricks/data00
  file: 
    path: /gluster_bricks/data00/data00
    owner: root
    group: root
    mode: 0775
    state: directory

# Create gluster data00 with new bricks and options
# Ansible host needs modules from galaxy: ansible-galaxy collection install gluster.gluster
# Broken: Creates gluster replication type as "distributed" and offers no set volume options needed to match what is set by oVirt
- name: create gluster volume data00 from servers and their bricks
  gluster.gluster.gluster_volume:
    state: present
    name: data00
    bricks: /gluster_bricks/data00/data00
    rebalance: yes
    cluster:
      - thorst.penguinpages.local
      - odinst.penguinpages.local
      - medusast.penguinpages.local
  run_once: true


# # ### Gluster options via CLI
# gluster volume set data00 cluster.self-heal-daemon enable
# gluster volume set data00 cluster.granular-entry-heal enable # this one complains about "cluster" aspect
# gluster volume set data00 performance.strict-o-direct on
# gluster volume set data00  storage.owner-gid 36
# gluster volume set data00 storage.owner-uid 36
# gluster volume set data00 server.keepalive-count 5
# gluster volume set data00 server.keepalive-interval 2
# gluster volume set data00 server.keepalive-time 10
# gluster volume set data00 server.tcp-user-timeout 20
# gluster volume set data00  network.ping-timeout 30
# gluster volume set data00  server.event-threads 4
# gluster volume set data00 client.event-threads 4
# gluster volume set data00 cluster.choose-local off
# gluster volume set data00 user.cifs off
# gluster volume set data00 features.shard on
# gluster volume set data00 cluster.shd-wait-qlength 10000
# gluster volume set data00 cluster.shd-max-threads 8
# gluster volume set data00 cluster.locking-scheme granular
# gluster volume set data00 cluster.data-self-heal-algorithm full
# gluster volume set data00 cluster.server-quorum-type server
# gluster volume set data00 cluster.quorum-type auto
# gluster volume set data00 cluster.eager-lock enable
# gluster volume set data00 network.remote-dio off
# gluster volume set data00 performance.low-prio-threads 32
# gluster volume set data00 performance.io-cache off
# gluster volume set data00 performance.read-ahead off
# gluster volume set data00 performance.quick-read off
# gluster volume set data00 storage.fips-mode-rchecksum on
# gluster volume set data00 transport.address-family inet
# gluster volume set data00  nfs.disable on
# gluster volume set data00 performance.client-io-threads on

#  Post gluster setup to define HCI logical netowrk for gluster
# https://access.redhat.com/documentation/en-us/red_hat_hyperconverged_infrastructure_for_virtualization/1.8/html-single/deploying_red_hat_hyperconverged_infrastructure_for_virtualization/index#configure-gluster-rhv-manager
# Broken:


# Mount gluster volume data00 on each HCI host in directory / mount path for use by server for backups / misc
# /etc/fstab #thorst.penguinpages.local:/data00 /media/data00 glusterfs defaults,backupvolfile-server=odinst.penguinpages.local:medusast.penguinpages.local,_netdev,log-level=WARNING,log-file=/var/log/gluster_data00.log 0 0



#####
#####

# - debug:
#     msg: "{{ gluster_brick_data00 }}"
# - debug: 
#     msg: "{{ gluster_brick_dir }}"
# - debug: 
#     msg: "{{ groups.hci_storage | join(',') }}"
# - debug:
#     msg: "{{ inventory_hostname }}"
# Create gluster volume and mount bricks.  https://www.jeffgeerling.com/blog/simple-glusterfs-setup-ansible
# - name: Configure Gluster volume data00 4:1 ratio compression 3replica.
#   gluster_volume:
#     state: present
#     name: "{{ gluster_brick_data00 }}"
#     brick: "{{ gluster_brick_dir }}"
#     replicas: 3
#     cluster: "{{ groups.hci_storage | join(',') }}"
#     host: "{{ inventory_hostname }}"
#     force: yes
#   run_once: true



# - name: Set multiple options on GlusterFS volume
#   gluster.gluster.gluster_volume:
#     state: present
#     name: data00
#     options:
#       { performance.cache-size: 128MB,
#         write-behind: 'off',
#         quick-read: 'on'
#       }

#  Firewall for gluster is a mess.  Need to think on how this would work
# Setup firewall rules for gluster server and client
# 24007 – Gluster Daemon
# 24008 – Management
# 24009 and greater (GlusterFS versions less than 3.4) OR
# 49152 (GlusterFS versions 3.4 and later) – Each brick for every volume on your host requires it’s own port. 
# For every new brick, one new port will be used starting at 24009 for GlusterFS versions below 3.4 and 49152 for version 3.4 and above. 
# If you have one volume with two bricks, you will need to open 24009 – 24010 (or 49152 – 49153).
# 38465 – 38467 – this is required if you by the Gluster NFS service.
# The following ports are TCP and UDP:
# 111 – portmapper
# - name: Allow PortMapper
#   firewalld:
#     port: 111/tcp
#     permanent: yes
#     state: enabled
# - name: Allow Gluster Daemon
#   firewalld:
#     port: 24007/tcp
#     permanent: yes
#     state: enabled
# - name: Allow Management
#   firewalld:
#     port: 24008/tcp
#     permanent: yes
#     state: enabled
# - name: Allow Gluster Daemon
#   firewalld:
#     port: 24009/tcp
#     permanent: yes
#     state: enabled

# Setup gluster peers 
# Broken.. does not like variable hci_storage 
# - name: Form a cluster for {{hci_storage}}
#   command: "gluster peer probe {{ hci_storage }}"
#   loop: "{{ hci_storage | flatten(levels=1) }}"
#   loop_control:
#     loop_var: peernode
#     pause: 2
#   register: glusterpeerprobe
#   become: true
#   run_once: true
#   any_errors_fatal: true
#   changed_when: "'peer probe: success.' in glusterpeerprobe.stdout_lines"

# - name: Add mount gluster volume for data with primary on local host and secondary 
#   mount:
#     name: /media/data
#     src: odinst.penguinpages.local:/data
#     opts: "defaults,backupvolfile-server=medusast.penguinpages.local:thorst.penguinpages.local,_netdev,log-level=WARNING,log-file=/var/log/gluster_data.log"
#     fstype: glusterfs
#     state: mounted
#   become: true

# - name: Add mount gluster volume for engine with primary on local host and secondary 
#   mount:
#     name: /media/engine
#     src: odinst.penguinpages.local:/engine
#     opts: "defaults,backupvolfile-server=medusast.penguinpages.local:thorst.penguinpages.local,_netdev,log-level=WARNING,log-file=/var/log/gluster_engine.log"
#     fstype: glusterfs
#     state: mounted
#   become: true

# - name: Add mount gluster volume for gv0 with primary on local host and secondary 
#   mount:
#     name: /media/gv0
#     src: odinst.penguinpages.local:/gv0
#     opts: "defaults,backupvolfile-server=medusast.penguinpages.local:thorst.penguinpages.local,_netdev,log-level=WARNING,log-file=/var/log/gluster_gv0.log"
#     fstype: glusterfs
#     state: mounted
#   become: true

# - name: Add mount gluster volume for vmstore with primary on local host and secondary 
#   mount:
#     name: /media/vmstore
#     src: odinst.penguinpages.local:/vmstore
#     opts: "defaults,backupvolfile-server=medusast.penguinpages.local:thorst.penguinpages.local,_netdev,log-level=WARNING,log-file=/var/log/gluster_vmstore.log"
#     fstype: glusterfs
#     state: mounted
#   become: true





# ############# Output from HCI deployment wizard to create Volumes #########
# hc_nodes:
#   hosts:
#     thorst.penguinpages.local:
#       gluster_infra_volume_groups:
#         - vgname: gluster_vg_sdb
#           pvname: /dev/mapper/vdo_sdb
#       gluster_infra_mount_devices:
#         - path: /gluster_bricks/engine
#           lvname: gluster_lv_engine
#           vgname: gluster_vg_sdb
#         - path: /gluster_bricks/data
#           lvname: gluster_lv_data
#           vgname: gluster_vg_sdb
#         - path: /gluster_bricks/vmstore
#           lvname: gluster_lv_vmstore
#           vgname: gluster_vg_sdb
#       gluster_infra_vdo:
#         - name: vdo_sdb
#           device: /dev/sdb
#           slabsize: 32G
#           logicalsize: 11000G
#           blockmapcachesize: 128M
#           emulate512: 'off'
#           writepolicy: auto
#           maxDiscardSize: 16M
#       blacklist_mpath_devices:
#         - sdb
#       gluster_infra_thick_lvs:
#         - vgname: gluster_vg_sdb
#           lvname: gluster_lv_engine
#           size: 1000G
#       gluster_infra_thinpools:
#         - vgname: gluster_vg_sdb
#           thinpoolname: gluster_thinpool_gluster_vg_sdb
#           poolmetadatasize: 3G
#       gluster_infra_lv_logicalvols:
#         - vgname: gluster_vg_sdb
#           thinpool: gluster_thinpool_gluster_vg_sdb
#           lvname: gluster_lv_data
#           lvsize: 5000G
#         - vgname: gluster_vg_sdb
#           thinpool: gluster_thinpool_gluster_vg_sdb
#           lvname: gluster_lv_vmstore
#           lvsize: 5000G
#     odinst.penguinpages.local:
#       gluster_infra_volume_groups:
#         - vgname: gluster_vg_sdb
#           pvname: /dev/mapper/vdo_sdb
#       gluster_infra_mount_devices:
#         - path: /gluster_bricks/engine
#           lvname: gluster_lv_engine
#           vgname: gluster_vg_sdb
#         - path: /gluster_bricks/data
#           lvname: gluster_lv_data
#           vgname: gluster_vg_sdb
#         - path: /gluster_bricks/vmstore
#           lvname: gluster_lv_vmstore
#           vgname: gluster_vg_sdb
#       gluster_infra_vdo:
#         - name: vdo_sdb
#           device: /dev/sdb
#           slabsize: 32G
#           logicalsize: 11000G
#           blockmapcachesize: 128M
#           emulate512: 'off'
#           writepolicy: auto
#           maxDiscardSize: 16M
#       blacklist_mpath_devices:
#         - sdb
#       gluster_infra_thick_lvs:
#         - vgname: gluster_vg_sdb
#           lvname: gluster_lv_engine
#           size: 1000G
#       gluster_infra_thinpools:
#         - vgname: gluster_vg_sdb
#           thinpoolname: gluster_thinpool_gluster_vg_sdb
#           poolmetadatasize: 3G
#       gluster_infra_lv_logicalvols:
#         - vgname: gluster_vg_sdb
#           thinpool: gluster_thinpool_gluster_vg_sdb
#           lvname: gluster_lv_data
#           lvsize: 5000G
#         - vgname: gluster_vg_sdb
#           thinpool: gluster_thinpool_gluster_vg_sdb
#           lvname: gluster_lv_vmstore
#           lvsize: 5000G
#     medusast.penguinpages.local:
#       gluster_infra_volume_groups:
#         - vgname: gluster_vg_sdb
#           pvname: /dev/mapper/vdo_sdb
#       gluster_infra_mount_devices:
#         - path: /gluster_bricks/engine
#           lvname: gluster_lv_engine
#           vgname: gluster_vg_sdb
#         - path: /gluster_bricks/data
#           lvname: gluster_lv_data
#           vgname: gluster_vg_sdb
#         - path: /gluster_bricks/vmstore
#           lvname: gluster_lv_vmstore
#           vgname: gluster_vg_sdb
#       gluster_infra_vdo:
#         - name: vdo_sdb
#           device: /dev/sdb
#           slabsize: 32G
#           logicalsize: 11000G
#           blockmapcachesize: 128M
#           emulate512: 'off'
#           writepolicy: auto
#           maxDiscardSize: 16M
#       blacklist_mpath_devices:
#         - sdb
#       gluster_infra_thick_lvs:
#         - vgname: gluster_vg_sdb
#           lvname: gluster_lv_engine
#           size: 1000G
#       gluster_infra_thinpools:
#         - vgname: gluster_vg_sdb
#           thinpoolname: gluster_thinpool_gluster_vg_sdb
#           poolmetadatasize: 3G
#       gluster_infra_lv_logicalvols:
#         - vgname: gluster_vg_sdb
#           thinpool: gluster_thinpool_gluster_vg_sdb
#           lvname: gluster_lv_data
#           lvsize: 5000G
#         - vgname: gluster_vg_sdb
#           thinpool: gluster_thinpool_gluster_vg_sdb
#           lvname: gluster_lv_vmstore
#           lvsize: 5000G
#   vars:
#     gluster_infra_disktype: JBOD
#     gluster_set_selinux_labels: true
#     gluster_infra_fw_ports:
#       - 2049/tcp
#       - 54321/tcp
#       - 5900/tcp
#       - 5900-6923/tcp
#       - 5666/tcp
#       - 16514/tcp
#     gluster_infra_fw_permanent: true
#     gluster_infra_fw_state: enabled
#     gluster_infra_fw_zone: public
#     gluster_infra_fw_services:
#       - glusterfs
#     gluster_features_force_varlogsizecheck: false
#     cluster_nodes:
#       - thorst.penguinpages.local
#       - odinst.penguinpages.local
#       - medusast.penguinpages.local
#     gluster_features_hci_cluster: '{{ cluster_nodes }}'
#     gluster_features_hci_volumes:
#       - volname: engine
#         brick: /gluster_bricks/engine/engine
#         arbiter: 0
#       - volname: data
#         brick: /gluster_bricks/data/data
#         arbiter: 0
#       - volname: vmstore
#         brick: /gluster_bricks/vmstore/vmstore
#         arbiter: 0

